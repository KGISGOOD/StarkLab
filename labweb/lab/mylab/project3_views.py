from django.shortcuts import render
import pandas as pd
import os
import sqlite3
import requests
from bs4 import BeautifulSoup
import time
import csv
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime, timedelta
import json
import threading
from django.views.decorators.http import require_GET
from django.http import JsonResponse
from .models import News
from django.views.decorators.csrf import csrf_exempt
import pandas as pd
import re
from datetime import datetime, timedelta
from collections import defaultdict
import random
import psutil


# ä¿®æ”¹ ALLOWED_SOURCES ç‚ºåªåŒ…å«å››å®¶å ±ç¤¾
ALLOWED_SOURCES = {
    'Newtalkæ–°è',
    'ç¶“æ¿Ÿæ—¥å ±',
    'è‡ªç”±æ™‚å ±',
    'BBC News ä¸­æ–‡'
}

# å®šç¾©å…è¨±çš„è‡ªç„¶ç½å®³é—œéµå­—
DISASTER_KEYWORDS = {
    'å¤§é›¨', 'è±ªé›¨', 'æš´é›¨', 'æ·¹æ°´', 'æ´ªæ°´', 'æ°´ç½',
    'é¢±é¢¨', 'é¢¶é¢¨', 'é¢¨ç½',
    'åœ°éœ‡', 'æµ·å˜¯',
    'ä¹¾æ—±', 'æ—±ç½', 'å¤§ç«', 'é‡ç«'
}

# é—œéµå­—è¨­å®š - ç”¨æ–¼åˆ¤æ–·åœ‹å…§æ–°è
domestic_keywords = [
    'å°ç£', 'å°åŒ—', 'æ–°åŒ—', 'åŸºéš†', 'æ–°ç«¹å¸‚', 'æ¡ƒåœ’', 'æ–°ç«¹ç¸£', 'å®œè˜­',
    'å°ä¸­', 'è‹—æ —', 'å½°åŒ–', 'å—æŠ•', 'é›²æ—', 'é«˜é›„', 'å°å—', 'å˜‰ç¾©',
    'å±æ±', 'æ¾æ¹–', 'èŠ±æ±', 'èŠ±è“®', 'å°9ç·š', 'é‡‘é–€', 'é¦¬ç¥–', 'ç¶ å³¶', 'è˜­å¶¼',
    'è‡ºç£', 'å°åŒ—', 'è‡ºä¸­', 'è‡ºå—', 'è‡º9ç¸£', 'å…¨å°', 'å…¨è‡º'
]

# è¨­å®š AI API æœ€å¤§é‡è©¦æ¬¡æ•¸å’Œåˆå§‹å»¶é²æ™‚é–“
max_retries = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸
retry_delay = 2  # åˆå§‹å»¶é²2ç§’

# å¾ Google News æŠ“å–è³‡æ–™ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼Œä½¿ç”¨ requests å’Œ BeautifulSoupï¼‰
def fetch_news(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        articles = soup.find_all(['article', 'div'], class_=['IFHyqb', 'xrnccd', 'IBr9hb', 'NiLAwe'])
        news_list = []
        allowed_sources_set = set(ALLOWED_SOURCES)

        for article in articles:
            try:
                title_element = article.find(['a', 'h3', 'h4'], class_=['JtKRv', 'ipQwMb', 'DY5T1d', 'gPFEn']) or article.find('a', recursive=True)
                if not title_element:
                    continue

                title = title_element.get_text(strip=True)
                link = title_element.get('href', '')
                link = f'https://news.google.com/{link[2:]}' if link.startswith('./') else f'https://news.google.com{link}' if link.startswith('/') else link

                source_element = article.find(['div', 'a'], class_=['vr1PYe', 'wEwyrc', 'SVJrMe', 'NmQAAc']) or article.find(lambda tag: tag.name in ['div', 'a'] and 'BBC' in tag.get_text())
                if not source_element:
                    continue

                source_name = source_element.get_text(strip=True)
                source_name = 'BBC News ä¸­æ–‡' if 'BBC' in source_name else source_name

                if source_name not in allowed_sources_set:
                    continue

                time_element = article.find(['time', 'div'], class_=['UOVeFe', 'hvbAAd', 'WW6dff', 'LfVVr'])
                date_str = time_element.get_text(strip=True) if time_element else 'æœªçŸ¥'
                date = parse_date(date_str)

                news_item = {
                    'æ¨™é¡Œ': title,
                    'é€£çµ': link,
                    'ä¾†æº': source_name,
                    'æ™‚é–“': date
                }
                news_list.append(news_item)

            except Exception as e:
                print(f"è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue

        return news_list

    except Exception as e:
        print(f"æŠ“å–æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return []

# å¾ Google News æŠ“å–è³‡æ–™ï¼ˆåƒ…ç”¨æ–¼ç¬¬ä¸€å€‹ URLï¼ŒåŒ…å«æ‰“é–‹ã€å»¶é²å’Œåˆ·æ–°ï¼‰
def fetch_news_with_refresh(url, driver):
    try:
        # å…ˆæ‰“é–‹ç¶²é 
        driver.get(url)
        print(f"å·²æ‰“é–‹ç¶²é : {url}")

        # éš¨æ©Ÿå»¶é² 2 åˆ° 3 ç§’
        delay_seconds = random.uniform(2, 3)
        print(f"â³ ç­‰å¾… {delay_seconds:.2f} ç§’å¾Œåˆ·æ–°ç¶²é ...")
        time.sleep(delay_seconds)

        # åˆ·æ–°ç¶²é 
        driver.refresh()
        print(f"å·²åˆ·æ–°ç¶²é : {url}")

        # ç­‰å¾…æ–°èå…ƒç´ åŠ è¼‰
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article"))
        )

        allowed_sources_set = set(ALLOWED_SOURCES)
        news_list = []

        # æŠ“å–æ–°èé …ç›®
        articles = driver.find_elements(By.CSS_SELECTOR, "article")
        for article in articles:
            try:
                title_element = article.find_element(By.CSS_SELECTOR, "h3, h4, a.JtKRv, a.ipQwMb, a.DY5T1d, a.gPFEn")
                if not title_element:
                    continue
                title = title_element.text.strip()
                link = title_element.get_attribute("href")
                if not link:
                    continue
                link = f'https://news.google.com/{link[2:]}' if link.startswith('./') else f'https://news.google.com{link}' if link.startswith('/') else link

                source_element = article.find_element(By.CSS_SELECTOR, "div.vr1PYe, a.wEwyrc, div.SVJrMe, div.NmQAAc")
                if not source_element:
                    continue
                source_name = source_element.text.strip()
                source_name = 'BBC News ä¸­æ–‡' if 'BBC' in source_name else source_name

                if source_name not in allowed_sources_set:
                    continue

                time_element = article.find_element(By.CSS_SELECTOR, "time, div.UOVeFe, div.hvbAAd, div.WW6dff, div.LfVVr")
                date_str = time_element.text.strip() if time_element else 'æœªçŸ¥'
                date = parse_date(date_str)

                news_item = {
                    'æ¨™é¡Œ': title,
                    'é€£çµ': link,
                    'ä¾†æº': source_name,
                    'æ™‚é–“': date
                }
                news_list.append(news_item)

            except Exception as e:
                print(f"è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue

        return news_list

    except Exception as e:
        print(f"æŠ“å–æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return []

# è§£æ Google News ä¸Šçš„æ—¥æœŸå­—ç¬¦ä¸²
def parse_date(date_str):
    current_date = datetime.now()
    
    if 'å¤©å‰' in date_str:
        days_ago = int(re.search(r'\d+', date_str).group())
        date = current_date - timedelta(days=days_ago)
    elif 'å°æ™‚å‰' in date_str:
        hours_ago = int(re.search(r'\d+', date_str).group())
        date = current_date - timedelta(hours=hours_ago)
    elif 'åˆ†é˜å‰' in date_str:
        minutes_ago = int(re.search(r'\d+', date_str).group())
        date = current_date - timedelta(minutes=minutes_ago)
    elif 'æ˜¨å¤©' in date_str:
        date = current_date - timedelta(days=1)
    else:
        try:
            if 'å¹´' not in date_str:
                date_str = f'{current_date.year}å¹´{date_str}'
            date = datetime.strptime(date_str, '%Yå¹´%mæœˆ%dæ—¥')
        except ValueError:
            date = current_date

    return date.strftime('%Y-%m-%d')

# è¨­ç½® Chrome é©…å‹•
def setup_chrome_driver():
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--disable-software-rasterizer')
    chrome_options.add_argument('--ignore-certificate-errors')
    chrome_options.add_argument('--ignore-ssl-errors')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# ç²å–æœ€çµ‚ç¶²å€ï¼ˆè™•ç† Google News è·³è½‰ï¼‰
def get_final_url(driver, url):
    try:
        driver.get(url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, 'a'))
        )
        final_url = driver.current_url
        return final_url
    except Exception as e:
        print(f"ç²å–æœ€çµ‚ç¶²å€å¤±æ•—: {e}")
        return url

# çˆ¬å–æ–‡ç« å…§å®¹
def fetch_article_content(driver, sources_urls):
    results = {}
    summaries = {}
    final_urls = {}

    content_selectors = {
        'Newtalkæ–°è': 'div.articleBody.clearfix p',
        'ç¶“æ¿Ÿæ—¥å ±': 'section.article-body__editor p',
        'è‡ªç”±æ™‚å ±': 'div.text p',
        'BBC News ä¸­æ–‡': 'div.bbc-1cvxiy9 p'
    }

    for source_name, url in sources_urls.items():
        if source_name not in ALLOWED_SOURCES:
            continue

        try:
            final_url = get_final_url(driver, url)
            final_urls[source_name] = final_url

            driver.get(final_url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'p'))
            )

            selector = content_selectors.get(source_name)
            if not selector:
                continue

            paragraphs = driver.find_elements(By.CSS_SELECTOR, selector)
            content = '\n'.join(p.text.strip() for p in paragraphs if p.text.strip())
            summary = content[:100] if content else 'æœªæ‰¾åˆ°å…§å®¹'

            results[source_name] = content if content else 'æœªæ‰¾åˆ°å…§å®¹'
            summaries[source_name] = summary

        except Exception as e:
            print(f"æŠ“å–å…§å®¹å¤±æ•—: {e}")
            results[source_name] = 'éŒ¯èª¤'
            summaries[source_name] = 'éŒ¯èª¤'
            final_urls[source_name] = url

    return results, summaries, final_urls

# çˆ¬å–åœ–ç‰‡ URL
def extract_image_url(driver, sources_urls):
    results = {}
    
    image_selectors = {
        'Newtalkæ–°è': "div.news_img img",
        'ç¶“æ¿Ÿæ—¥å ±': "section.article-body__editor img",
        'è‡ªç”±æ™‚å ±': "div.image-popup-vertical-fit img",
        'BBC News ä¸­æ–‡': "div.bbc-1cvxiy9 img"
    }

    for source_name, url in sources_urls.items():
        if source_name not in ALLOWED_SOURCES:
            continue
            
        try:
            driver.get(url)
            selector = image_selectors.get(source_name)
            if not selector:
                continue
            
            if source_name == 'BBC News ä¸­æ–‡':
                try:
                    content_div = driver.find_element(By.CSS_SELECTOR, 'div.bbc-1cvxiy9')
                    if content_div:
                        first_image = content_div.find_element(By.TAG_NAME, 'img')
                        if first_image and 'src' in first_image.get_attribute('outerHTML'):
                            results[source_name] = first_image.get_attribute('src')
                            continue
                except Exception as e:
                    print(f"ç„¡æ³•æ‰¾åˆ° BBC æ–°èåœ–ç‰‡: {e}")

            image_element = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            
            image_url = image_element.get_attribute('src') or image_element.get_attribute('data-src')
            results[source_name] = image_url or ''
            
        except Exception as e:
            print(f"åœ–ç‰‡æ“·å–éŒ¯èª¤: {e}")
            results[source_name] = ''
            
    return results

# åˆå§‹åŒ–è¿½è¹¤æœ€é«˜è¨˜æ†¶é«”ä½¿ç”¨é‡
max_mem_mb = 0

#  RAM å¤§å°æ¸¬é‡
def log_memory_usage(note=''):
    global max_mem_mb
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024 / 1024
    print(f"[Memory] {note} ä½¿ç”¨è¨˜æ†¶é«”ï¼š{mem_mb:.2f} MB")
    max_mem_mb = max(max_mem_mb, mem_mb)
    return mem_mb

# CSV æª”æ¡ˆå¤§å°æ¸¬é‡
def get_file_size(filepath):
    try:
        size_bytes = os.path.getsize(filepath)
        size_kb = size_bytes / 1024
        size_mb = size_kb / 1024
        print(f"[ğŸ“¦ æª”æ¡ˆå¤§å°] {filepath}ï¼š{size_kb:.2f} KB / {size_mb:.2f} MB")
        return size_mb
    except Exception as e:
        print(f"âŒ ç„¡æ³•å–å¾—æª”æ¡ˆå¤§å°ï¼š{e}")
        return 0

# çˆ¬èŸ²ä¸»å‡½æ•¸
@require_GET
def crawler_first_stage(request):
    try:
        global max_mem_mb
        total_start_time = time.time()  # âœ… è¨˜éŒ„ç¸½é–‹å§‹æ™‚é–“
        day = "7"
        log_memory_usage("é–‹å§‹çˆ¬èŸ²")
        
        # Google News æœå°‹ URL
        urls = [
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E5%A4%A7%E9%9B%A8%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›å¤§é›¨
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E8%B1%AA%E9%9B%A8%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›è±ªé›¨
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%9A%B4%E9%9B%A8%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›æš´é›¨
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B7%B9%E6%B0%B4%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›æ·¹æ°´
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B4%AA%E6%B0%B4%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›æ´ªæ°´
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B0%B4%E7%81%BD%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›æ°´ç½    
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E9%A2%B1%E9%A2%A8%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›é¢±é¢¨
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E9%A2%B6%E9%A2%A8%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›é¢¶é¢¨    
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E9%A2%A8%E7%81%BD%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›é¢¨ç½
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B5%B7%E5%98%AF%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›æµ·å˜¯
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E5%9C%B0%E9%9C%87%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›åœ°éœ‡
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E4%B9%BE%E6%97%B1%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›ä¹¾æ—±
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%97%B1%E7%81%BD%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›æ—±ç½
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E5%A4%A7%E7%81%AB%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›å¤§ç«ï¼é‡ç«
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E9%87%8E%E7%81%AB%20when%3A'+day+'d&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›é‡ç«
            # åŠ ä¸Šbbcé—œéµå­—
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E8%B1%AA%E9%9B%A8%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E5%A4%A7%E9%9B%A8%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%9A%B4%E9%9B%A8%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B7%B9%E6%B0%B4%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B4%AA%E6%B0%B4%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B0%B4%E7%81%BD%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E9%A2%B1%E9%A2%A8%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E9%A2%B6%E9%A2%A8%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E9%A2%A8%E7%81%BD%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B5%B7%E5%98%AF%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E5%9C%B0%E9%9C%87%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E6%B5%B7%E5%98%AF%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›æµ·å˜¯
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E5%9C%B0%E9%9C%87%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›åœ°éœ‡
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E5%A4%A7%E7%81%AB%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant',#åœ‹éš›å¤§ç«ï¼é‡ç«
            'https://news.google.com/search?q=%E5%9C%8B%E9%9A%9B%E9%87%8E%E7%81%AB%20when%3A'+day+'d%20bbc&hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant'#åœ‹éš›é‡ç«      
        ]
        
        log_memory_usage("åˆå§‹åŒ– Chrome é©…å‹•å‰")
        driver = setup_chrome_driver()
        log_memory_usage("åˆå§‹åŒ– Chrome é©…å‹•å¾Œ")

        all_news_items = []
        start_crawl_time = time.time()

        if urls:
            first_url = urls[0]
            news_items = fetch_news_with_refresh(first_url, driver)
            all_news_items.extend(news_items)
            log_memory_usage("ç¬¬ä¸€å€‹ URL çˆ¬å®Œå¾Œ")

        for url in urls[1:]:
            news_items = fetch_news(url)
            all_news_items.extend(news_items)
            log_memory_usage(f"çˆ¬å®Œ URL: {url}")

        if all_news_items:
            log_memory_usage("æº–å‚™å»ºç«‹ DataFrame")
            news_df = pd.DataFrame(all_news_items)
            news_df = news_df.drop_duplicates(subset='æ¨™é¡Œ', keep='first')
            log_memory_usage("å»ºç«‹ä¸¦å»é‡ DataFrame å¾Œ")

            end_crawl_time = time.time()
            crawl_time = int(end_crawl_time - start_crawl_time)
            hours, remainder = divmod(crawl_time, 3600)
            minutes, seconds = divmod(remainder, 60)

            time_str = ''
            if hours > 0:
                time_str += f'{hours}å°æ™‚'
            if minutes > 0 or hours > 0:
                time_str += f'{minutes}åˆ†'
            time_str += f'{seconds}ç§’'

            # print(f'Google News çˆ¬å–å®Œæˆï¼Œè€—æ™‚ï¼š{time_str}')

            first_stage_file = 'w2.csv'
            if os.path.exists(first_stage_file):
                os.remove(first_stage_file)

            for index, item in news_df.iterrows():
                source_name = item['ä¾†æº']
                original_url = item['é€£çµ']
                sources_urls = {source_name: original_url}

                content_results, _, final_urls = fetch_article_content(driver, sources_urls)
                image_results = extract_image_url(driver, sources_urls)

                content = content_results.get(source_name, '')
                final_url = final_urls.get(source_name, original_url)
                image_url = image_results.get(source_name, '')

                result = {
                    'æ¨™é¡Œ': item['æ¨™é¡Œ'],
                    'é€£çµ': final_url,
                    'å…§æ–‡': content or '',
                    'ä¾†æº': source_name,
                    'æ™‚é–“': item['æ™‚é–“'],
                    'åœ–ç‰‡': image_url or ''
                }

                output_df = pd.DataFrame([result])
                # log_memory_usage(f"å„²å­˜æ–°èï¼š{item['æ¨™é¡Œ']} å‰")
                output_df.to_csv(first_stage_file, mode='a', header=not os.path.exists(first_stage_file),
                                 index=False, encoding='utf-8')
                log_memory_usage(f"å„²å­˜æ–°èï¼š{item['æ¨™é¡Œ']} å¾Œ")

                print(f"å·²å„²å­˜æ–°è: {result['æ¨™é¡Œ']}")

            driver.quit()
            final_mem_mb = log_memory_usage("çˆ¬èŸ²çµæŸå‰")
            csv_size_mb = get_file_size(first_stage_file)

            total_end_time = time.time()
            total_elapsed = total_end_time - total_start_time
            total_minutes = int(total_elapsed // 60)
            total_seconds = total_elapsed % 60
            total_time_str = f'{total_minutes} åˆ† {total_seconds:.2f} ç§’'
            print(f'ğŸ•’ ç¬¬ä¸€éšæ®µçˆ¬èŸ²ç¸½è€—æ™‚ï¼š{total_time_str}')

            return JsonResponse({
                'status': 'success',
                'message': f'ç¬¬ä¸€éšæ®µçˆ¬èŸ²å®Œæˆï¼è€—æ™‚ï¼š{time_str}',
                'total_elapsed_time': total_time_str,
                'csv_file': first_stage_file,
                'total_news': len(news_df),
                'final_memory_mb': f'{final_mem_mb:.2f} MB',
                'peak_memory_mb': f'{max_mem_mb:.2f} MB',
                'csv_file_size_mb': f'{csv_size_mb:.2f} MB'
            })

        else:
            driver.quit()
            total_end_time = time.time()
            total_elapsed = total_end_time - total_start_time
            total_minutes = int(total_elapsed // 60)
            total_seconds = total_elapsed % 60
            total_time_str = f'{total_minutes} åˆ† {total_seconds:.2f} ç§’'
            print(f'âš ï¸ æ²’æœ‰æ‰¾åˆ°æ–°èï¼Œç¸½è€—æ™‚ï¼š{total_time_str}')

            return JsonResponse({
                'status': 'error',
                'message': 'æ²’æœ‰æ‰¾åˆ°æ–°è',
                'total_elapsed_time': total_time_str
            })

    except Exception as e:
        if 'driver' in locals():
            driver.quit()

        total_end_time = time.time()
        total_elapsed = total_end_time - total_start_time
        total_minutes = int(total_elapsed // 60)
        total_seconds = total_elapsed % 60
        total_time_str = f'{total_minutes} åˆ† {total_seconds:.2f} ç§’'

        print(f'âŒ çˆ¬èŸ²éŒ¯èª¤æ™‚ç¸½è€—æ™‚ï¼š{total_time_str}')

        return JsonResponse({
            'status': 'error',
            'message': f'çˆ¬èŸ²åŸ·è¡Œå¤±æ•—ï¼š{str(e)}',
            'total_elapsed_time': total_time_str
        }, status=500)

#ai è™•ç†
#å°å…¥api key
from mylab.config import xai_api_key, model_name


def news_ai(request):

    def chat_with_xai(prompt, api_key, model_name, context=""):
        
        for attempt in range(max_retries):
            try:
                url = 'https://api.x.ai/v1/chat/completions'
                headers = {
                    'Content-Type': 'application/json',
                    'Authorization': f'Bearer {api_key}'
                }
    
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ–°èåˆ†æåŠ©æ‰‹"},
                    {"role": "user", "content": prompt}
                ]
    
                data = {
                    "messages": messages,
                    "model": model_name,
                    "temperature": 0,
                    "stream": False
                }
    
                response = requests.post(url, headers=headers, json=data, timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    if result and 'choices' in result and result['choices']:
                        content = result['choices'][0]['message']['content']
                        return content
                elif response.status_code == 429:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                        print(f"API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                        time.sleep(wait_time)
                        continue
                
                print(f"API èª¿ç”¨å¤±æ•— (ç‹€æ…‹ç¢¼: {response.status_code})")
                return "ç„¡æ³•å–å¾—å›æ‡‰"  # æ”¹ç‚ºæœ‰æ„ç¾©çš„é è¨­å€¼
    
            except Exception as e:
                print(f"API éŒ¯èª¤: {str(e)}")
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    print(f"ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
                    continue
                return "ç™¼ç”ŸéŒ¯èª¤"  # æ”¹ç‚ºæœ‰æ„ç¾©çš„é è¨­å€¼
    
    #é˜²å‘†æ©Ÿåˆ¶ï¼Œåˆªé™¤å…§æ–‡æ˜¯éŒ¯èª¤çš„æ–°è
    # è®€å– CSV
    df = pd.read_csv('w2.csv')
    
    # ç¯©é¸å‡º 'å…§æ–‡' æ¬„ä½ä¸æ˜¯ "éŒ¯èª¤" çš„è³‡æ–™
    cleaned_df = df[df['å…§æ–‡'] != 'éŒ¯èª¤'].copy()

    # å­˜å› CSVï¼ˆä¸åŒ…å«ç´¢å¼•ï¼‰
    cleaned_df.to_csv('w2.csv', index=False)
    print(f"å·²æˆåŠŸæ¸…é™¤å«æœ‰éŒ¯èª¤å…§æ–‡çš„æ–°èï¼Œå…±åˆªé™¤ {len(df) - len(cleaned_df)} ç­†è³‡æ–™ã€‚")

            
    #1.æ°´åˆ©ç½²_ç¢ºèªæ˜¯å¦ç½å®³
    # åŠ å…¥é‡è©¦æ©Ÿåˆ¶åƒæ•¸
    def is_disaster_news(title, content):
        """
        ä½¿ç”¨ X.AI åˆ¤æ–·æ–°èæ˜¯å¦ä¸»è¦å ±å°è‡ªç„¶ç½å®³äº‹ä»¶
        """
        # ç¢ºä¿ `content` æ˜¯å­—ä¸²ï¼Œé¿å… TypeError
        content = str(content)  
        
        prompt = f"""
        è«‹åˆ¤æ–·ä»¥ä¸‹æ–°èæ˜¯å¦ä¸»è¦åœ¨å ±å°è‡ªç„¶ç½å®³äº‹ä»¶æœ¬èº«ï¼Œåªéœ€å›ç­” true æˆ– falseï¼š
        
        å…è¨±çš„ç½å®³é¡å‹ï¼šå¤§é›¨ã€è±ªé›¨ã€æš´é›¨ã€æ·¹æ°´ã€æ´ªæ°´ã€æ°´ç½ã€é¢±é¢¨ã€é¢¶é¢¨ã€é¢¨ç½ã€åœ°éœ‡ã€æµ·å˜¯ã€ä¹¾æ—±ã€æ—±ç½ã€é‡ç«

        æ–°èæ¨™é¡Œï¼š{title}
        æ–°èå…§å®¹ï¼š{content[:500]}

        åˆ¤æ–·æ¨™æº–ï¼š
        1. æ–°èå¿…é ˆä¸»è¦æè¿°ç½å®³äº‹ä»¶æœ¬èº«ï¼ŒåŒ…æ‹¬ï¼š
        - ç½å®³çš„ç™¼ç”Ÿéç¨‹
        - ç½å®³é€ æˆçš„ç›´æ¥å½±éŸ¿å’Œæå¤±
        - ç½å®³ç¾å ´çš„æƒ…æ³æè¿°

        2. ä»¥ä¸‹é¡å‹çš„æ–°èéƒ½å›ç­”falseï¼š
        - ç½å¾Œæ´åŠ©æˆ–æè´ˆæ´»å‹•çš„å ±å°
        - åœ‹éš›æ•‘æ´è¡Œå‹•çš„æ–°è
        - ç½å¾Œé‡å»ºç›¸é—œå ±å°
        - é˜²ç½æ”¿ç­–è¨è«–
        - æ°£å€™è®Šé·è­°é¡Œ
        - æ­·å²ç½å®³å›é¡§
        - ä»¥ç½å®³ç‚ºèƒŒæ™¯ä½†ä¸»è¦å ±å°å…¶ä»–äº‹ä»¶çš„æ–°è
        - ç„¦é»åœ¨æ–¼åäººã€å¥¢è¯ç”Ÿæ´»æˆ–æ”¿æ²»äººç‰©çš„ç½å¾Œåæ‡‰æ–°è
        - ä»¥ç½å®³ç‚ºèƒŒæ™¯ï¼Œä¸»è¦å ±å°è²¡ç”¢æå¤±æˆ–å¥¢è¯ç‰©å“ï¼ˆå¦‚è±ªå®…ã€å¥§é‹çç‰Œç­‰ï¼‰çš„æ–°è
        - é—œæ–¼ç½å¾Œåäººå½±éŸ¿ã€è²¡ç”¢æå¤±çš„å ±å°ï¼Œä¾‹å¦‚é—œæ–¼æ˜æ˜Ÿæˆ–åäººå®¶åœ’è¢«ç‡’æ¯€çš„å ±å°
        - ä¸»è¦å ±å°ç½å¾Œæ”¿åºœæˆ–æ”¿æ²»äººç‰©çš„åæ‡‰ã€æ±ºç­–æˆ–è¡Œå‹•çš„æ–°è
        - ä¸»è¦å ±å°ç½å®³å¾Œçš„å…¬å…±å¥åº·å»ºè­°ã€ç•¶å±€æŒ‡ç¤ºæˆ–é é˜²æªæ–½ï¼ˆå¦‚é˜²ç¯„æªæ–½ã€é…æˆ´å£ç½©ã€N95ç­‰ï¼‰æ–°è
        - å…§æ–‡ç„¡äººå“¡å‚·äº¡æˆ–æ˜¯è²¡å‹™æå¤±
        - è¾²ä½œç‰©ç”¢é‡åŠ‡æ¸›ã€æ¸›å°‘ã€æå¤±ï¼Œæ¶æ•‘å‹•ç‰©
        
        3. ç‰¹åˆ¥æ³¨æ„ï¼š
        - å¦‚æœæ–°èä¸»è¦åœ¨å ±å°æ•‘æ´ã€æåŠ©ã€å¤–äº¤ç­‰æ´»å‹•ï¼Œå³ä½¿æåˆ°ç½å®³ä¹Ÿæ‡‰è©²å›ç­” false
        - å¦‚æœæ–°èåªæ˜¯ç”¨ç½å®³ä½œç‚ºèƒŒæ™¯ï¼Œä¸»è¦å ±å°å…¶ä»–äº‹ä»¶ï¼Œæ‡‰è©²å›ç­” false
        - æ–°èçš„æ ¸å¿ƒä¸»é¡Œå¿…é ˆæ˜¯ç½å®³äº‹ä»¶æœ¬èº«æ‰å›ç­” true
        - æ—¥æœ¬åœ°éœ‡å ±å°äº‹ä»¶ï¼Œæ‡‰è©²å›ç­”true
        - ã€Œå€Ÿæˆ‘èº²ä¸€ä¸‹ï¼ã€ ç‚ºé¿åŠ å·é‡ç« 238å…¬æ–¤é»‘ç†Šã€Œå·´é‡Œã€èº²åˆ°æ°‘å®…åœ°æ¿ä¸‹ false

        """

        for attempt in range(max_retries):
            try:
                response = chat_with_xai(prompt, xai_api_key, model_name, "")
                return 'true' in response.lower()
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                    print(f"API éŒ¯èª¤: {str(e)}. ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
                else:
                    print(f"API éŒ¯èª¤: {str(e)}. å·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ã€‚")
                    return False  # æˆ–è€…å¯ä»¥è¿”å›å…¶ä»–åˆé©çš„å€¼ä¾†è¡¨ç¤ºå¤±æ•—

    # 1. è®€å– CSV æª”æ¡ˆ
    df = pd.read_csv('w2.csv')

    # 2. é€è¡Œåˆ¤æ–·æ˜¯å¦ç‚ºç½å®³æ–°èï¼Œä¸¦æ–°å¢æ¬„ä½
    df['is_disaster'] = df.apply(lambda row: is_disaster_news(row['æ¨™é¡Œ'], str(row['å…§æ–‡'])), axis=1)

    # 3. éæ¿¾åªä¿ç•™ is_disaster ç‚º True çš„è¡Œ
    df_true = df[df['is_disaster'] == True]

    # 4. å°‡çµæœå­˜å„²åˆ°æ–°çš„ CSV æª”æ¡ˆ
    print(df_true)
    df_true.to_csv('true_new.csv', index=False, encoding='utf-8-sig')



    #2.æ°´åˆ©ç½²ï¼¿å¾æ–°èå…§æ–‡ä¸­æå–ä¸‰å€‹è³‡è¨Šæ¬„ä½ï¼šåœ‹å®¶ã€åœ°é» å’Œ ç½å®³
    def extract_information(news_content):
        """
        ä½¿ç”¨ AI æå–åœ‹å®¶ã€åœ°é»å’Œç½å®³ä¸‰å€‹æ¬„ä½ï¼Œæ ¹æ“šæ–°èå…§æ–‡ç”Ÿæˆã€‚
        """
        prompt = f"""
        è«‹æ ¹æ“šä»¥ä¸‹å…§æ–‡æ¬„ä½æå–æ‰€æœ‰ç›¸é—œçš„åœ‹å®¶ã€åœ°é»å’Œç½å®³ï¼š
        å…è¨±çš„ç½å®³é¡å‹ï¼šå¤§é›¨ã€è±ªé›¨ã€æš´é›¨ã€æ·¹æ°´ã€æ´ªæ°´ã€æ°´ç½ã€é¢±é¢¨ã€é¢¶é¢¨ã€é¢¨ç½ã€åœ°éœ‡ã€æµ·å˜¯ã€ä¹¾æ—±ã€æ—±ç½ã€é‡ç«
        
        æª¢æ ¸æ¨™æº–ï¼š
        - åœ‹å®¶æ˜¯å¦å®Œæ•´ï¼Œåªèƒ½æœ‰ä¸€å€‹åœ‹å®¶
        - åœ°é»æ˜¯å¦å®Œæ•´(ä¸éºæ¼ä»»ä½•æåˆ°çš„åœ°é»ï¼Œå¯ä»¥åŒ…å«å¤šå€‹åœ°é»)
        - ç½å®³æ˜¯å¦å®Œæ•´ï¼Œåªèƒ½æœ‰ä¸€å€‹ï¼Œä¸¦ä¸”å¿…é ˆåªèƒ½æ˜¯å…è¨±çš„ç½å®³é¡å‹ã€‚å¦‚æœç„¡æ³•ç¢ºå®šå…·é«”é¡å‹ï¼Œè«‹å°‡ç½å®³æ­¸é¡ç‚ºæœ€ç›¸ä¼¼çš„å…è¨±ç½å®³
        - æ ¼å¼æ˜¯å¦ä¸€è‡´(æ¯å€‹å­—ä¸²ä¸€å€‹é …ç›®)
        - æè¿°æ˜¯å¦æº–ç¢º(åœ°ç†ä½ç½®æº–ç¢ºæ€§)
        
        ç‰¹åˆ¥æ³¨æ„ï¼š
        - å¦‚æœå‡ºç¾åƒæ˜¯ ç«å±±å™´ç™¼ ç­‰ä¸æ˜¯å…è¨±çš„ç½å®³é¡å‹çš„ç½å®³ï¼Œå‰‡ä¾ç…§å…§æ–‡æ•˜è¿°å°‡å…¶æ­¸é¡åˆ°æœ€ç›¸ä¼¼çš„å…è¨±ç½å®³ï¼Œä¾‹å¦‚é‡ç«
        - å¦‚æœå‡ºç¾åƒæ˜¯ æ´ªæ°´,æ°´ç½,é¢±é¢¨ å¤šå€‹å…è¨±ç½å®³å‡ºç¾ï¼Œå‰‡ä¾ç…§å…§æ–‡æ•˜è¿°å°‡å…¶æ­¸é¡åˆ°æœ€ç›¸ä¼¼çš„å…è¨±ç½å®³ï¼Œä¾‹å¦‚é¢±é¢¨
        - å¦‚æœå‡ºç¾åƒæ˜¯ æ³•åœ‹,é¦¬é”åŠ æ–¯åŠ ,è«ä¸‰æ¯”å…‹ ä¸‰å€‹åœ‹å®¶ï¼Œå‰‡ä¾ç…§å…§æ–‡æ•˜è¿°å°‡å…¶æ­¸é¡åˆ°ä¸€å€‹åœ‹å®¶ï¼Œä¾‹å¦‚æ³•åœ‹
        
        è«‹ç›´æ¥è¼¸å‡ºä»¥ä¸‹æ ¼å¼(ç”¨æ›è¡Œå€åˆ†):
        åœ‹å®¶: ["åœ‹å®¶1"]
        åœ°é»: ["åœ°é»1", "åœ°é»2"]
        ç½å®³: ["ç½å®³1"]
        
        æ–°èå…§å®¹:
        {news_content}
        """
        
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                # å‡è¨­ chat_with_xai æ˜¯æ•´åˆ AI çš„å‡½æ•¸
                response = chat_with_xai(prompt, xai_api_key, model_name, "")
                
                # æ‰“å° AI å›å‚³çš„å…§å®¹ä»¥é€²è¡Œæª¢æŸ¥
                print("AI å›å‚³å…§å®¹:", response)

                # åˆ†æçµæœæå–
                response_lines = response.strip().split("\n")
                result = {"åœ‹å®¶": "", "åœ°é»": "", "ç½å®³": ""}

                for line in response_lines:
                    key, _, value = line.partition(":")  # åˆ†å‰²å‡ºéµå’Œå€¼
                    if key.strip() == "åœ‹å®¶":
                        result["åœ‹å®¶"] = value.strip().strip('[]"').replace('\", \"', ',')
                    elif key.strip() == "åœ°é»":
                        result["åœ°é»"] = value.strip().strip('[]"').replace('\", \"', ',')
                    elif key.strip() == "ç½å®³":
                        result["ç½å®³"] = value.strip().strip('[]"').replace('\", \"', ',')
                
                return result
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                    print(f"API éŒ¯èª¤: {str(e)}. ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
                else:
                    print(f"API éŒ¯èª¤: {str(e)}. å·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ã€‚")
                    return {"åœ‹å®¶": "", "åœ°é»": "", "ç½å®³": ""}  # è¿”å›ç©ºçµæœè¡¨ç¤ºå¤±æ•—

    # è®€å–è³‡æ–™
    df = pd.read_csv('true_new.csv')  # é€™æ˜¯åŸå§‹æª”æ¡ˆï¼ŒåŒ…å«ã€Œå…§æ–‡ã€æ¬„ä½

    # æ ¹æ“šå…§æ–‡æ¬„ä½ç”Ÿæˆåœ‹å®¶ã€åœ°é»å’Œç½å®³ï¼Œä¸¦å°‡å…¶å­˜æ”¾åˆ°æ–°çš„æ¬„ä½
    df[['åœ‹å®¶', 'åœ°é»', 'ç½å®³']] = df['å…§æ–‡'].apply(lambda text: pd.Series(extract_information(text)))

    # å°‡çµæœå¯«å…¥æ–°çš„ CSV æª”æ¡ˆ
    df.to_csv('add_locations.csv', index=False, encoding='utf-8')

    print("è³‡è¨Šç”Ÿæˆå®Œæˆï¼Œå·²å„²å­˜ç‚º add_locations.csv")


    #3.æ°´åˆ©ç½²_event
    def extract_information(news_title, news_content):
        prompt = f"""
        eventæ¬„ä½æ ¹æ“šè³‡æ–™é›†æ–°èæ¨™é¡Œå’Œå…§æ–‡ï¼Œåˆ¤æ–·æ˜¯å¦å ±å°ç›¸åŒçš„ç½å®³äº‹ä»¶ï¼Œä¸¦åˆ†é…ä¸€è‡´çš„äº‹ä»¶åç¨±ã€‚
        
        æª¢æ ¸æ¨™æº–ï¼š
        - å¿…é ˆåŒæ™‚ä½¿ç”¨æ–°èæ¨™é¡Œå’Œå…§æ–‡ä¾†åˆ¤æ–·æ˜¯å¦ç‚ºç›¸åŒçš„ç½å®³äº‹ä»¶ã€‚
        - è‹¥æ¨™é¡Œå’Œå…§æ–‡æè¿°çš„ç½å®³äº‹ä»¶ç›¸åŒï¼ˆå³æ¶‰åŠç›¸åŒç½å®³é¡å‹ã€æ™‚é–“ç¯„åœï¼‰ï¼Œå‰‡å¿…é ˆåˆ†é…ç›¸åŒçš„äº‹ä»¶åç¨±ã€‚
        - è‹¥æ¨™é¡Œå’Œå…§æ–‡æ¶‰åŠä¸åŒçš„ç½å®³äº‹ä»¶ï¼ˆä¾‹å¦‚ä¸åŒæ™‚é–“æˆ–ç½å®³é¡å‹ï¼‰ï¼Œå‰‡æ‡‰åˆ†é…ä¸åŒçš„äº‹ä»¶åç¨±ã€‚
        - ç½å®³é¡å‹åŒ…å«ï¼šå¤§é›¨ã€è±ªé›¨ã€æš´é›¨ã€æ·¹æ°´ã€æ´ªæ°´ã€æ°´ç½ã€é¢±é¢¨ã€é¢¶é¢¨ã€é¢¨ç½ã€åœ°éœ‡ã€æµ·å˜¯ã€ä¹¾æ—±ã€æ—±ç½ã€é‡ç«ã€‚
        - contentæ¬„ä½æ ¹æ“šå…§æ–‡ç”Ÿæˆ50-100å­—çš„æ‘˜è¦ï¼Œéœ€ç²¾ç¢ºåæ˜ ç½å®³çš„æ ¸å¿ƒä¿¡æ¯ã€‚
        - summaryæ¬„ä½æ ¹æ“šå…§æ–‡ç”Ÿæˆæå¤±èˆ‡ç½å®³çš„çµ±æ•´ï¼Œéœ€åŒ…å«å…·é«”æå¤±æ•¸æ“šï¼ˆå¦‚æ­»äº¡äººæ•¸ã€æ’¤é›¢äººæ•¸ã€è²¡ç”¢æå¤±ï¼‰åŠç½å®³å½±éŸ¿ç¯„åœã€‚
        - ç¬¬ä¸€éšæ®µï¼ševentæ¬„ä½åªç”Ÿæˆã€Œåœ‹å®¶+ç½å®³é¡å‹ã€ï¼Œä¸åŒ…å«åœ°é»ã€‚
        - æ™‚é–“ç¯„åœçš„åˆ¤æ–·ï¼šè‹¥ç½å®³äº‹ä»¶æŒçºŒå¤šæ—¥ï¼Œæ‡‰è¦–ç‚ºåŒä¸€äº‹ä»¶ï¼Œé™¤éæ˜ç¢ºæåˆ°ä¸åŒçš„ç½å®³ç™¼ç”Ÿã€‚
        - åœ‹å®¶åç¨±æ¨™æº–åŒ–ï¼šå°‡ã€ŒéŸ“åœ‹ã€çµ±ä¸€è½‰ç‚ºã€Œå—éŸ“ã€ï¼Œä¾‹å¦‚ã€ŒéŸ“åœ‹+é‡ç«ã€æ‡‰è¼¸å‡ºç‚ºã€Œå—éŸ“+é‡ç«ã€ï¼›å…¶ä»–åœ‹å®¶åç¨±ä¿æŒæ¨™æº–åŒ–ï¼Œå¦‚ã€Œå°ç£ã€ã€ã€Œç¾åœ‹ã€ã€ã€Œæ—¥æœ¬ã€ç­‰ã€‚
        - ç½å®³é¡å‹æ¨™æº–åŒ–ï¼šå°‡ã€Œæš´é›¨ã€çµ±ä¸€è½‰ç‚ºã€Œå¤§é›¨ã€ï¼Œä¾‹å¦‚ã€Œå°ç£+æš´é›¨ã€æ‡‰è¼¸å‡ºç‚ºã€Œå°ç£+å¤§é›¨ã€ã€‚
        
        ç”Ÿæˆeventæ™‚æ³¨æ„ï¼š
        - åœ‹å®¶ï¼šä½¿ç”¨æ¨™æº–åœ‹å®¶åç¨±ï¼Œä¸¦éµå¾ªä»¥ä¸‹æ¨™æº–åŒ–è¦å‰‡ï¼š
        - ã€ŒéŸ“åœ‹ã€è½‰ç‚ºã€Œå—éŸ“ã€ã€‚
        - ã€Œå°ç£ã€ã€ã€Œç¾åœ‹ã€ã€ã€Œæ—¥æœ¬ã€ç­‰ä¿æŒä¸è®Šã€‚
        - ç½å®³é¡å‹ï¼šä½¿ç”¨æª¢æ ¸æ¨™æº–ä¸­çš„åç¨±ï¼Œä¸¦å°‡ã€Œæš´é›¨ã€è½‰ç‚ºã€Œå¤§é›¨ã€ï¼Œé¿å…ä½¿ç”¨åŒç¾©è©æˆ–è®Šé«”ã€‚
        
        è«‹ç›´æ¥è¼¸å‡ºä»¥ä¸‹æ ¼å¼(ç”¨æ›è¡Œå€åˆ†):
        event: "åœ‹å®¶+ç½å®³é¡å‹"
        content: "<50-100å­—æ‘˜è¦>"
        summary: "<æå¤±èˆ‡ç½å®³çš„çµ±æ•´>"
        
        æ–°èæ¨™é¡Œ:
        {news_title}
        
        æ–°èå…§å®¹:
        {news_content}
        """

        response = chat_with_xai(prompt, xai_api_key, model_name, "")
        print("AI å›å‚³å…§å®¹:", response)

        response_lines = response.strip().split("\n")
        result = {
            "event": "",
            "content": "",
            "summary": ""
        }

        for line in response_lines:
            key, _, value = line.partition(":")
            if key == "event":
                event = value.strip().strip('"').replace(" ", "")
                if "+" in event and len(event.split("+")) == 2:
                    country, disaster = event.split("+")
                    # ç½å®³é¡å‹æ¨™æº–åŒ–ï¼šå°‡ã€Œæš´é›¨ã€è½‰ç‚ºã€Œå¤§é›¨ã€
                    if disaster == "æš´é›¨":
                        disaster = "å¤§é›¨"
                    result["event"] = f"{country}+{disaster}"
                else:
                    result["event"] = event  # ä¿ç•™åŸå§‹å€¼ä½†ä¸è™•ç†
            elif key == "content":
                result["content"] = value.strip().strip('"')
            elif key == "summary":
                result["summary"] = value.strip().strip('"')
        
        return result

    # ç¬¬ä¸€éšæ®µï¼šç”Ÿæˆåˆæ­¥çš„ eventï¼ˆåªåŒ…å«åœ‹å®¶å’Œç½å®³é¡å‹ï¼‰
    df = pd.read_csv('add_locations.csv')

    # å‡è¨­ add_locations.csv å·²åŒ…å« 'åœ°é»' æ¬„ä½ï¼Œè‹¥ç„¡æ­¤æ¬„ä½éœ€é¡å¤–è™•ç†
    if 'åœ°é»' not in df.columns:
        raise ValueError("è¼¸å…¥æª”æ¡ˆ 'add_locations.csv' ç¼ºå°‘ 'åœ°é»' æ¬„ä½")

    df['åˆ†æçµæœ'] = df.apply(lambda row: extract_information(row['æ¨™é¡Œ'], row['å…§æ–‡']), axis=1)
    df['event'] = df['åˆ†æçµæœ'].apply(lambda x: x['event'])
    df['content'] = df['åˆ†æçµæœ'].apply(lambda x: x['content'])
    df['summary'] = df['åˆ†æçµæœ'].apply(lambda x: x['summary'])
    df = df.drop(columns=['åˆ†æçµæœ'])

    # ç¬¬äºŒéšæ®µï¼šåˆ†çµ„ä¸¦æ›´æ–° eventï¼Œé¸æ“‡å–®ä¸€åœ°é»ä¸¦ç§»é™¤ "+"
    event_groups = defaultdict(list)

    # å°‡è³‡æ–™æŒ‰ event åˆ†çµ„
    for index, row in df.iterrows():
        event = row['event']
        if event and "+" in event and len(event.split("+")) == 2:  # æª¢æŸ¥æ ¼å¼ç‚º "åœ‹å®¶+ç½å®³é¡å‹"
            country, disaster = event.split("+")
            # è‹¥ 'åœ°é»' æ¬„ä½åŒ…å«å¤šå€‹åœ°é»ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰ï¼Œåˆ†å‰²ä¸¦å–ç¬¬ä¸€å€‹
            location = row['åœ°é»']
            if pd.notna(location) and ',' in str(location):
                location = str(location).split(',')[0].strip()  # å–ç¬¬ä¸€å€‹åœ°é»
            event_groups[(country, disaster)].append((index, location))
        else:
            print(f"è­¦å‘Š: ç¬¬ä¸€éšæ®µ event æ ¼å¼ä¸æ­£ç¢º - {event}")

    # çµ±è¨ˆæ¯çµ„ä¸­åœ°é»çš„å‡ºç¾æ¬¡æ•¸ä¸¦æ›´æ–° event
    for (country, disaster), group in event_groups.items():
        # çµ±è¨ˆåœ°é»é »ç‡
        loc_count = defaultdict(int)
        for _, location in group:
            if pd.notna(location):  # ç¢ºä¿åœ°é»ä¸æ˜¯ NaN
                loc_count[location] += 1
        
        # æ‰¾å‡ºå‡ºç¾æ¬¡æ•¸æœ€å¤šçš„åœ°é»
        if loc_count:
            max_count = max(loc_count.values())
            most_common_locations = [loc for loc, count in loc_count.items() if count == max_count]
            chosen_location = most_common_locations[0]  # é¸æ“‡ç¬¬ä¸€å€‹å‡ºç¾çš„åœ°é»
        else:
            chosen_location = "æœªçŸ¥"  # è‹¥ç„¡æœ‰æ•ˆåœ°é»ï¼Œè¨­ç‚º "æœªçŸ¥"
        
        # æ›´æ–°è©²çµ„çš„ eventï¼Œæ ¼å¼ç‚º "åœ‹å®¶åœ°é»ç½å®³é¡å‹"
        new_event = f"{country}{chosen_location}{disaster}"
        for index, _ in group:
            df.at[index, 'event'] = new_event

    # å„²å­˜çµæœ
    df.to_csv('add_events.csv', index=False, encoding='utf-8')

    print("è³‡è¨Šç”Ÿæˆå®Œæˆï¼Œå·²å„²å­˜ç‚º add_events.csv")

    #4.æ°´åˆ©ç½²ï¼¿region
    # åœ‹å…§é—œéµå­—æ¸…å–®
    domestic_keywords = [
        'å°ç£', 'å°åŒ—', 'æ–°åŒ—', 'åŸºéš†', 'æ–°ç«¹å¸‚', 'æ¡ƒåœ’', 'æ–°ç«¹ç¸£', 'å®œè˜­', 
        'å°ä¸­', 'è‹—æ —', 'å½°åŒ–', 'å—æŠ•', 'é›²æ—', 'é«˜é›„', 'å°å—', 'å˜‰ç¾©', 
        'å±æ±', 'æ¾æ¹–', 'èŠ±æ±', 'èŠ±è“®', 'å°9ç·š', 'é‡‘é–€', 'é¦¬ç¥–', 'ç¶ å³¶', 'è˜­å¶¼',
        'è‡ºç£', 'å°åŒ—', 'è‡ºä¸­', 'è‡ºå—', 'è‡º9ç¸£', 'å…¨å°', 'å…¨è‡º'
    ]

    # åŒ¯å…¥ CSV æª”æ¡ˆ
    input_file = 'add_events.csv'  # æ›¿æ›æˆä½ çš„æª”æ¡ˆåç¨±

    try:
        # è®€å– CSV æª”æ¡ˆ
        df = pd.read_csv(input_file)

        # ç¢ºä¿å…§æ–‡æ¬„ä½å­˜åœ¨
        if 'åœ°é»' not in df.columns:
            raise ValueError("CSV æª”æ¡ˆä¸­æ²’æœ‰ 'å…§æ–‡' æ¬„ä½")

        # æ–°å¢ region æ¬„ä½
        def determine_region(content):
            is_domestic = any(keyword in content for keyword in domestic_keywords)
            return 'åœ‹å…§' if is_domestic else 'åœ‹å¤–'

        # ä½¿ç”¨ apply æ–¹æ³•å°æ¯å‰‡æ–°èé€²è¡Œåˆ¤æ–·
        df['region'] = df['åœ°é»'].apply(determine_region)

        # å°‡çµæœå­˜å›æ–°çš„ CSV æª”æ¡ˆ
        output_file = 'region.csv'
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"æ–°å¢æ¬„ä½ region å®Œæˆï¼Œçµæœå·²å„²å­˜åˆ° {output_file}")

    except Exception as e:
        print(f"è™•ç†éç¨‹ä¸­å‡ºç¾éŒ¯èª¤ï¼š{str(e)}")

    #7.æ°´åˆ©ç½²_overview
    # è§£ææ¨¡ç³Šæ™‚é–“ï¼ˆå¦‚ã€Œä»Šæ—¥ã€ã€ã€Œæ˜¨æ—¥ã€ï¼‰
    def process_relative_dates(text, reference_date):
        if not isinstance(reference_date, str) or not reference_date.strip():
            return text  # è‹¥ç„¡å¯ç”¨çš„åƒè€ƒæ—¥æœŸï¼Œå‰‡ä¸ä¿®æ”¹

        try:
            reference_date = datetime.strptime(reference_date, "%Y-%m-%d")  # è½‰æ›ç‚º datetime ç‰©ä»¶
        except ValueError:
            return text  # è‹¥æ—¥æœŸè§£æå¤±æ•—å‰‡ä¸ä¿®æ”¹æ–‡æœ¬

        replacements = {
            r"\bä»Šæ—¥\b": reference_date.strftime("%Y-%m-%d"),
            r"\bä»Šå¤©\b": reference_date.strftime("%Y-%m-%d"),
            r"\bæ˜¨æ—¥\b": (reference_date - timedelta(days=1)).strftime("%Y-%m-%d"),
            r"\bæ˜¨å¤©\b": (reference_date - timedelta(days=1)).strftime("%Y-%m-%d"),
            r"\bå‰å¤©\b": (reference_date - timedelta(days=2)).strftime("%Y-%m-%d"),
        }

        for pattern, value in replacements.items():
            text = re.sub(pattern, value, text)

        return text

    def extract_explicit_date(text):
        """å¾å…§æ–‡ä¸­æå–æ˜ç¢ºçš„ YYYYå¹´MMæœˆDDæ—¥ æ ¼å¼æ™‚é–“"""
        date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
        if date_match:
            year, month, day = date_match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        return None

    def extract_relative_disaster_date(text, reference_date):
        """å¾å…§æ–‡ä¸­æå–ç›¸å°æ™‚é–“ä¸¦è½‰æ›ç‚ºæ¨™æº–æ—¥æœŸï¼Œåƒ…é‡å°ç½å®³ç™¼ç”Ÿæ™‚é–“"""
        try:
            ref_date = datetime.strptime(reference_date, "%Y-%m-%d")
        except ValueError:
            return None

        relative_patterns = {
            r"\b(ä»Šæ—¥|ä»Šå¤©)(å‡Œæ™¨|ä¸Šåˆ|ä¸‹åˆ|æ™šä¸Š)?\s*(\d{1,2}æ™‚\d{1,2}åˆ†)?\s*(ç™¼ç”Ÿ|æœ‰)": ref_date,
            r"\b(æ˜¨æ—¥|æ˜¨å¤©)(å‡Œæ™¨|ä¸Šåˆ|ä¸‹åˆ|æ™šä¸Š)?\s*(\d{1,2}æ™‚\d{1,2}åˆ†)?\s*(ç™¼ç”Ÿ|æœ‰)": ref_date - timedelta(days=1),
            r"\bå‰å¤©(å‡Œæ™¨|ä¸Šåˆ|ä¸‹åˆ|æ™šä¸Š)?\s*(\d{1,2}æ™‚\d{1,2}åˆ†)?\s*(ç™¼ç”Ÿ|æœ‰)": ref_date - timedelta(days=2),
        }

        for pattern, date in relative_patterns.items():
            if re.search(pattern, text):
                return date.strftime("%Y-%m-%d")
        return None

    def has_disaster_time(text):
        """åˆ¤æ–·å…§æ–‡æ˜¯å¦åŒ…å«ç½å®³ç™¼ç”Ÿçš„æ™‚é–“ï¼ˆæ¨™æº–æ ¼å¼æˆ–ç›¸å°æ—¥æœŸï¼‰"""
        # æª¢æŸ¥æ¨™æº–æ—¥æœŸæ ¼å¼
        if re.search(r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥', text):
            return True
        # æª¢æŸ¥ç›¸å°æ—¥æœŸé—œéµè©ä¸¦èˆ‡ç½å®³ç›¸é—œ
        relative_patterns = [
            r"\b(ä»Šæ—¥|ä»Šå¤©|æ˜¨æ—¥|æ˜¨å¤©|å‰å¤©)(å‡Œæ™¨|ä¸Šåˆ|ä¸‹åˆ|æ™šä¸Š)?\s*(\d{1,2}æ™‚\d{1,2}åˆ†)?\s*(ç™¼ç”Ÿ|æœ‰)"
        ]
        for pattern in relative_patterns:
            if re.search(pattern, text):
                return True
        return False

    def generate_overview(group):
        """é‡å° event ç¾¤çµ„ç”Ÿæˆ summary çš„ç¸½çµ"""
        reference_date = group['æ™‚é–“'].dropna().astype(str).min()  # å–å¾—æœ€æ—©çš„æ™‚é–“

        group['summary'] = group['summary'].apply(lambda x: process_relative_dates(x, reference_date) if isinstance(x, str) else x)
        group['å…§æ–‡'] = group['å…§æ–‡'].apply(lambda x: process_relative_dates(x, reference_date) if isinstance(x, str) else x)

        explicit_dates = group['å…§æ–‡'].dropna().apply(extract_explicit_date).dropna()
        relative_dates = group['å…§æ–‡'].dropna().apply(lambda x: extract_relative_disaster_date(x, reference_date)).dropna()
        
        # å„ªå…ˆä½¿ç”¨æ˜ç¢ºæ—¥æœŸï¼Œè‹¥ç„¡å‰‡ä½¿ç”¨ç›¸å°æ—¥æœŸï¼Œæœ€å¾Œç”¨åƒè€ƒæ—¥æœŸ
        overview_date = explicit_dates.min() if not explicit_dates.empty else \
                        relative_dates.min() if not relative_dates.empty else reference_date

        combined_content = " ".join(group['summary'].dropna()) + " " + " ".join(group['å…§æ–‡'].dropna())

        if not combined_content.strip():
            return "ç„¡æ³•ç”Ÿæˆæ‘˜è¦ï¼Œè³‡æ–™ä¸è¶³"

        # æª¢æŸ¥å…§æ–‡æ˜¯å¦åŒ…å«ç½å®³æ™‚é–“
        has_time = any(group['å…§æ–‡'].dropna().apply(has_disaster_time))

        prompt = f"""
        æ ¹æ“šä»¥ä¸‹æ‰€æœ‰ç›¸é—œäº‹ä»¶çš„æ‘˜è¦ï¼ˆsummaryï¼‰å’Œå…§æ–‡ï¼Œç”Ÿæˆä¸€å€‹æœ‰åœ‹å®¶åœ°é»ç½å®³ç¸½æ•´ç†çš„ç½å®³è³‡è¨Šæ‘˜è¦ï¼ˆoverviewï¼‰ã€‚
        
        è«‹éµå¾ªä»¥ä¸‹è¦å‰‡ï¼š
        - è‹¥å…§æ–‡æ˜ç¢ºæåˆ°ç½å®³ç™¼ç”Ÿçš„æ™‚é–“ï¼ˆå¦‚ 2025å¹´1æœˆ12æ—¥ï¼‰ï¼Œå‰‡å°‡è©²æ™‚é–“æ”¾åœ¨æ‘˜è¦æœ€å‰é¢ã€‚
        - è‹¥å…§æ–‡æåˆ°ç›¸å°æ™‚é–“ï¼ˆå¦‚ã€Œä»Šå¤©å‡Œæ™¨5æ™‚30åˆ†ç™¼ç”Ÿã€ã€ã€Œæ˜¨æ—¥ç™¼ç”Ÿã€ï¼‰ä¸”èˆ‡ç½å®³ç›¸é—œï¼Œå‰‡åƒè€ƒ `æ™‚é–“` æ¬„ä½ï¼ˆ{reference_date}ï¼‰è½‰æ›ç‚ºæ¨™æº–æ—¥æœŸï¼Œä¸¦æ”¾åœ¨æ‘˜è¦æœ€å‰é¢ã€‚
        - è‹¥å…§æ–‡æ²’æœ‰æåˆ°ç½å®³ç™¼ç”Ÿçš„æ™‚é–“ï¼Œå‰‡ä¸è¦åœ¨æ‘˜è¦å‰é¢åŠ å…¥æ™‚é–“ã€‚
        - ç¢ºä¿ä½¿ç”¨çš„æ™‚é–“æ˜¯ç½å®³ç™¼ç”Ÿçš„æ™‚é–“ï¼Œè€Œéå…¶ä»–ç„¡é—œæ™‚é–“ï¼ˆå¦‚æ–°èç™¼å¸ƒæ™‚é–“ï¼‰ã€‚
        
        æª¢æ ¸æ¨™æº–ï¼š
        1. æ™‚é–“æº–ç¢ºï¼šè‹¥æœ‰æ™‚é–“ï¼Œå¿…é ˆæ˜¯ç½å®³ç™¼ç”Ÿçš„æ™‚é–“ã€‚
        2. å…§å®¹å®Œæ•´ï¼šæ‘˜è¦éœ€åŒ…å«åœ°é»ã€ç½å®³é¡å‹ã€å½±éŸ¿ç¯„åœåŠå¾ŒçºŒç™¼å±•ã€‚
        3. çµæ§‹æ¸…æ™°ï¼šè‹¥æ¶‰åŠå¤šå€‹äº‹ä»¶ï¼Œæ‡‰æŒ‰æ™‚é–“é †åºæˆ–é‡è¦æ€§æ•´ç†ã€‚
        4. å­—æ•¸é™åˆ¶ï¼šæ‘˜è¦é ˆæ§åˆ¶åœ¨ 100-150 å­—ã€‚
        
        äº‹ä»¶åƒè€ƒæ™‚é–“ï¼š{reference_date}
        å…§æ–‡æ˜¯å¦åŒ…å«ç½å®³æ™‚é–“ï¼š{has_time}
        ç½å®³ç™¼ç”Ÿæ™‚é–“ï¼ˆè‹¥æœ‰ï¼‰ï¼š{overview_date}
        
        ç›¸é—œäº‹ä»¶æ‘˜è¦ï¼ˆsummary å’Œ å…§æ–‡ï¼‰ï¼š
        {combined_content}
        
        ç¯„ä¾‹äº‹ä»¶æ‘˜è¦ï¼ˆsummaryï¼‰ï¼š
        1. 2024å¹´12æœˆ23æ—¥ï¼Œç¬¬26è™Ÿé¢±é¢¨å¸•å¸ƒç”Ÿæˆï¼Œé è¨ˆæœä¸­å—åŠå³¶æ–¹å‘ç§»å‹•ï¼Œå°å°ç£ç„¡ç›´æ¥å½±éŸ¿ï¼Œä½†å¤–åœæ°´æ°£å°‡å°è‡´å…¨å°è½‰é›¨ã€‚
        2. ä»Šå¤©å‡Œæ™¨5æ™‚30åˆ†ï¼Œå—å¤ªå¹³æ´‹å³¶åœ‹è¬é‚£æœç™¼ç”Ÿè¦æ¨¡7.4åœ°éœ‡ï¼Œéœ‡æºæ·±åº¦10å…¬é‡Œï¼Œéš¨å¾Œç™¼å¸ƒæµ·å˜¯è­¦å ±ã€‚
        
        è«‹ç›´æ¥è¼¸å‡ºï¼š
        overview: "<ç½å®³è³‡è¨Šæ‘˜è¦>"
        """

        response = chat_with_xai(prompt, xai_api_key, model_name, "")
        
        if response:
            overview_line = response.strip().split(":")
            clean_overview = overview_line[1].strip().strip('"').replace("*", "") if len(overview_line) > 1 else "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"
            return clean_overview
        return "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"

    # è®€å– CSV
    df = pd.read_csv('region.csv')

    # ç¢ºä¿ `event` æ¬„ä½ç‚ºåˆ†é¡ç¾¤çµ„
    df['event'] = df['event'].astype(str)

    # å…ˆç”Ÿæˆ overviewï¼Œå†åˆä½µå›åŸå§‹ df
    overview_df = df.groupby('event', group_keys=False).apply(generate_overview).reset_index()
    overview_df.columns = ['event', 'overview']

    # åˆä½µå› dfï¼Œç¢ºä¿ overview æ”¾åœ¨æ­£ç¢ºçš„ event ä¸Š
    df = df.merge(overview_df, on='event', how='left')

    # å„²å­˜çµæœ
    df.to_csv('add_overview.csv', index=False, encoding='utf-8')
    print("ä¿®æ­£å¾Œçš„ overview å·²å­˜å…¥ add_overview.csv")


    #8.æ°´åˆ©ç½²_åˆä½µ
    #è£œé½Šæ¬„ä½
    # è®€å– CSV æª”æ¡ˆ
    df = pd.read_csv('add_overview.csv')  

    # å®šç¾©æ¬„ä½åç¨±å°æ‡‰é—œä¿‚
    column_mapping = {
        'æ¨™é¡Œ': 'title',
        'é€£çµ': 'url',
        'ä¾†æº': 'publisher',
        'æ™‚é–“': 'date',
        'åœ–ç‰‡': 'cover'
    }

    # åŸ·è¡Œæ¬„ä½åç¨±æ›´æ”¹
    df = df.rename(columns=column_mapping)

    # 2. åˆªé™¤ä¸è¦çš„æ¬„ä½
    columns_to_drop = ['å…§æ–‡', 'is_disaster', 'ç½å®³']
    df = df.drop(columns=columns_to_drop, errors='ignore')  # errors='ignore' ç¢ºä¿å³ä½¿æ¬„ä½ä¸å­˜åœ¨ä¹Ÿä¸æœƒå ±éŒ¯

    # 3. è£œä¸Šç¼ºå¤±æ¬„ä½
    # recent_updateï¼šé¸æ“‡ date æ¬„ä½ä¸­æœ€æ–°çš„æ™‚é–“
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')  # ç¢ºä¿ date æ¬„ä½æ˜¯æ—¥æœŸæ ¼å¼
        df['recent_update'] = df['date'].max()  # é¸å–æœ€æ–°çš„æ™‚é–“
    else:
        df['recent_update'] = pd.NaT  # å¦‚æœ date æ¬„ä½ä¸å­˜åœ¨ï¼Œå¡«å…¥ NaTï¼ˆç¼ºå¤±å€¼ï¼‰

    # locationï¼šå°‡ã€Œåœ‹å®¶ã€å’Œã€Œåœ°é»ã€åˆä½µæˆä¸€å€‹æ¬„ä½
    if 'åœ‹å®¶' in df.columns and 'åœ°é»' in df.columns:
        df['location'] = df['åœ‹å®¶'].fillna('') + ' ' + df['åœ°é»'].fillna('')  # åˆä½µã€Œåœ‹å®¶ã€å’Œã€Œåœ°é»ã€ï¼Œä¸¦è™•ç†ç¼ºå¤±å€¼
        df['location'] = df['location'].str.strip()  # å»é™¤å¤šé¤˜çš„ç©ºæ ¼
    else:
        df['location'] = ''  # å¦‚æœã€Œåœ‹å®¶ã€æˆ–ã€Œåœ°é»ã€æ¬„ä½ä¸å­˜åœ¨ï¼Œå‰‡æ–°å¢ç©ºçš„ location æ¬„ä½

    # 4. æ–°å¢ author å’Œ publish_date æ¬„ä½
    # authorï¼šèˆ‡ publisher æ¬„ä½ç›¸åŒ
    if 'publisher' in df.columns:
        df['author'] = df['publisher']
    else:
        df['author'] = ''  # å¦‚æœ publisher æ¬„ä½ä¸å­˜åœ¨ï¼Œå‰‡å¡«å…¥ç©ºå­—ä¸²

    # publish_dateï¼šèˆ‡ date æ¬„ä½ç›¸åŒ
    if 'date' in df.columns:
        df['publish_date'] = df['date']
    else:
        df['publish_date'] = pd.NaT  # å¦‚æœ date æ¬„ä½ä¸å­˜åœ¨ï¼Œå‰‡å¡«å…¥ NaTï¼ˆç¼ºå¤±å€¼ï¼‰

    # 5. åˆªé™¤ã€Œåœ‹å®¶ã€å’Œã€Œåœ°é»ã€æ¬„ä½
    columns_to_drop_after_merge = ['åœ‹å®¶', 'åœ°é»']
    df = df.drop(columns=columns_to_drop_after_merge, errors='ignore')  # errors='ignore' ç¢ºä¿å³ä½¿æ¬„ä½ä¸å­˜åœ¨ä¹Ÿä¸æœƒå ±éŒ¯

    # 6.æ–°å¢æ­¥é©Ÿï¼šç§»é™¤ç›¸åŒ title çš„é‡è¤‡é …ç›®ï¼Œåªä¿ç•™ç¬¬ä¸€å€‹å‡ºç¾çš„
    if 'title' in df.columns:
        df = df.drop_duplicates(subset='title', keep='first')

    # 7. è¼¸å‡ºåˆ°æ–°çš„ CSV æª”æ¡ˆ
    output_file = 'è£œé½Šæ¬„ä½.csv'
    df.to_csv(output_file, index=False, encoding='utf-8')

    print(f"è™•ç†å®Œæˆï¼Œå·²è¼¸å‡ºåˆ° {output_file}")

    #åˆä½µæ¬„ä½
    # è®€å– CSV æª”æ¡ˆ
    df = pd.read_csv('è£œé½Šæ¬„ä½.csv')

    # åˆå§‹åŒ–ä¸€å€‹ç©ºçš„åˆ—è¡¨ï¼Œç”¨ä¾†å­˜æ”¾æœ€çµ‚çš„çµæ§‹
    result = []

    # æŒ‰ç…§ event é€²è¡Œåˆ†çµ„ï¼Œæ‰€æœ‰å…·æœ‰ç›¸åŒ 'event' å€¼çš„è¡Œæœƒè¢«åˆ†åˆ°åŒä¸€çµ„
    for event, group in df.groupby('event'):
        # é¸æ“‡ç¬¬ä¸€å€‹æ–°èçš„æ•¸æ“šä½œç‚ºåŸºæœ¬ä¿¡æ¯
        first_row = group.iloc[0]
        
        # è™•ç† cover æ¬„ä½ï¼Œè‹¥ç‚º NaN å‰‡è¨­ç‚ºç©ºå­—ä¸²
        cover = first_row['cover'] if pd.notna(first_row['cover']) else ""
        
        # æ‰¾åˆ°è©² event çµ„å…§æœ€æ–°çš„æ—¥æœŸä½œç‚º recent_update
        recent_update = group['date'].max()
        
        # æ‰¾åˆ°è©² event çµ„å…§æœ€æ—©çš„æ—¥æœŸä½œç‚º date
        earliest_date = group['date'].min()
        
        # åˆå§‹åŒ–ç•¶å‰äº‹ä»¶çš„å­—å…¸
        event_data = {
            "event": event,
            "region": first_row['region'],
            "cover": cover,
            "date": earliest_date,  # ä½¿ç”¨è©² event çµ„å…§æœ€æ—©çš„æ—¥æœŸ
            "recent_update": recent_update,  # ä½¿ç”¨è©² event çµ„å…§æœ€æ–°çš„æ—¥æœŸ
            "location": first_row['location'].split(',') if pd.notna(first_row['location']) else [],
            "overview": first_row['overview'],
            "daily_records": [],
            "links": []
        }

        # è™•ç† daily_recordsï¼Œéæ­·æ‰€æœ‰è³‡æ–™
        unique_daily_records = group[['date', 'content', 'location']].drop_duplicates()
        for _, row in unique_daily_records.iterrows():
            daily_record = {
                "date": row['date'],
                "content": row['content'],
                "location": row['location'].split(',') if pd.notna(row['location']) else []
            }
            event_data["daily_records"].append(daily_record)

        # æ’åº daily_records æŒ‰ç…§æ—¥æœŸç”±èˆŠåˆ°æ–°
        event_data["daily_records"].sort(key=lambda x: x["date"])

        # å»é™¤ links ä¸­ title é‡è¤‡çš„è³‡æ–™
        unique_links = group.drop_duplicates(subset=["title"])

        for _, row in unique_links.iterrows():
            link = {
                "source": {
                    "publisher": row['publisher'],
                    "author": row['author']
                },
                "url": row['url'],
                "title": row['title'],
                "publish_date": row['publish_date'],
                "location": row['location'].split(',') if pd.notna(row['location']) else [],
                "summary": row['summary']
            }
            event_data["links"].append(link)

        # å°‡æ¯å€‹äº‹ä»¶çš„æ•¸æ“šæ·»åŠ åˆ°çµæœåˆ—è¡¨ä¸­
        result.append(event_data)

    # å°‡çµæœåˆ—è¡¨è½‰æ›ç‚º JSON æ ¼å¼ä¸¦å¯«å…¥æª”æ¡ˆ
    with open('final.json', 'w', encoding='utf-8') as json_file:
        json.dump(result, json_file, ensure_ascii=False, indent=2)

    print("JSON æ–‡ä»¶å·²ç”Ÿæˆä¸¦å‘½åç‚º 'final.json'ã€‚")
    return JsonResponse({"message": "æ–°èAIé‹è¡Œå®Œæˆ"})

# æª”æ¡ˆè™•ç†
CSV_FILE_PATH = 'w2.csv'
JSON_FILE_PATH = 'final.json'

@require_GET
def view_raw_news(request):
    try:
        # å–å¾—è«‹æ±‚æ ¼å¼ (json æˆ– csv)ï¼Œé è¨­ç‚º json
        data_format = request.GET.get('format', 'json').lower()

        if data_format == 'csv':
            # æª¢æŸ¥ CSV æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not os.path.exists(CSV_FILE_PATH):
                return JsonResponse({'error': 'CSV æª”æ¡ˆä¸å­˜åœ¨'}, status=404)

            # è®€å– CSV æª”æ¡ˆ
            news_df = pd.read_csv(CSV_FILE_PATH)

            # æº–å‚™ JSON æ ¼å¼çš„æ–°èåˆ—è¡¨
            news_list = []
            for _, row in news_df.iterrows():
                content = row.get('å…§æ–‡', '') or ''
                if len(content) > 100:
                    content = content[:100] + '...'

                news_item = {
                    'ä¾†æº': row.get('ä¾†æº', '') or '',
                    'ä½œè€…': row.get('ä¾†æº', '') or '',
                    'æ¨™é¡Œ': row.get('æ¨™é¡Œ', '') or '',
                    'é€£çµ': row.get('é€£çµ', '') or '',
                    'å…§æ–‡': content,
                    'æ™‚é–“': row.get('æ™‚é–“', '') or '',
                    'åœ–ç‰‡': row.get('åœ–ç‰‡', '') or ''
                }
                news_list.append(news_item)

            return JsonResponse(news_list, safe=False, json_dumps_params={'ensure_ascii': False, 'indent': 4})

        else:
            # æª¢æŸ¥ JSON æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not os.path.exists(JSON_FILE_PATH):
                return JsonResponse({'error': 'JSON æª”æ¡ˆä¸å­˜åœ¨'}, status=404)

            # è®€å– JSON æª”æ¡ˆå…§å®¹
            with open(JSON_FILE_PATH, 'r', encoding='utf-8') as file:
                data = json.load(file)

            return JsonResponse(data, safe=False, json_dumps_params={'ensure_ascii': False, 'indent': 4})

    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)
    




# æ–°å¢å‡½æ•¸ run_crawler_and_ai
def run_crawler_and_ai(request):
    print("run_crawler_and_ai è¢«å‘¼å«")
    start_time = time.time()  # è¨˜éŒ„é–‹å§‹æ™‚é–“

    # å‘¼å«ç¬¬ä¸€éšæ®µçˆ¬èŸ²
    crawler_response = crawler_first_stage(request)
    if crawler_response.status_code != 200:
        return JsonResponse({'error': 'Crawler failed'}, status=500)

    # å‘¼å« AI è™•ç†
    ai_response = news_ai(request)
    if ai_response.status_code != 200:
        return JsonResponse({'error': 'AI failed'}, status=500)

    end_time = time.time()  # è¨˜éŒ„çµæŸæ™‚é–“
    elapsed_time = end_time - start_time  # è¨ˆç®—åŸ·è¡Œæ™‚é–“
    print(f"ç¸½åŸ·è¡Œæ™‚é–“ï¼š{elapsed_time:.2f} ç§’")

    return JsonResponse({
        'status': 'all done',
        'execution_time': f"{elapsed_time:.2f} ç§’",
        'crawler': crawler_response.content.decode(),
        'ai': ai_response.content.decode(),
    })